\section{Acceleration Performance Evaluation}
\label{sec:results_acceleration}

\begin{figure*}[ht]
\centering
\vspace{0pt}
\begin{minipage}[c]{.3\textwidth}
	\includegraphics[width=0.25\paperwidth]{graphs/obj/hammingfull-crop.pdf}
	\caption{Nearest Neighbor with FlashBoost up to Two Nodes}
	\label{fig:result_hammingfull}
\end{minipage}\hfill
\vspace{0pt}
\begin{minipage}[c]{.3\textwidth}
	\includegraphics[width=0.25\paperwidth]{graphs/obj/hamming-crop.pdf}
	\caption{Nearest Neighbor with Single-Node Throttled FlashBoost}
	\label{fig:result_hamming}
\end{minipage}\hfill
\vspace{0pt}
\begin{minipage}[c]{.3\textwidth}
	\includegraphics[width=0.25\paperwidth]{graphs/obj/graph-crop.pdf}
	\caption{Graph Traversal Performance}
	\label{fig:result_graph}
\end{minipage}
\end{figure*}

\subsection{Nearest Neighbor Search}

Figure~\ref{fig:result_hamming} shows the performance of nearest-neighbor search
with various data sources, normalized to the in-storage processing performance.
We compared FlashBoost against a high-cost fully DRAM configuration, as well as
realistic systems where some data cannot fit in DRAM.
Table~\ref{tab:nearest_neighbor} describes the benchmarks depicted in
Figure~\ref{fig:result_hamming}.

It should be noted that we have throttled our flash storage throughput to
600MB/s for this experiment, which is the bandwidth of the SATA 3.0
specification. This is to compare only the benefits of the in-store processing
architecture against other designs, otherwise the high bandwidth of the
FlashBoost hardware will result in an unfair comparison. When using all of the
2.4GB/s of our flash bandwidth, FlashBoost with ISP outperforms DRAM up to 4 threads.

\begin{tabular}{l | p{0.25\paperwidth}}
\label{tab:nearest_neighbor}
Name & Description \\
\hline \hline
DRAM & Store all data in DRAM \\
ISP & Process data in in-storage accelerator \\
FlashBoost+SW & Use FlashBoost as raw storage \\
Seq Flash & All requests are sequential flash accesses \\
10\% Flash & Store most data in DRAM. 10\% chance of hitting flash \\
5\% Disk & Store most data in DRAM. 5\% chance of hitting disk \\
Full Flash & All requests go to flash \\
\hline
\end{tabular}

It can be seen that streaming data directly from DRAM is obviously the fastest, and
scales linearly with thread count because it becomes a computation-bound
workload. The configuration that uses an in-storage processor to offload
computation is consistently faster than the software implementation, because
there is no software overhead involved, and the in-storage processor can process
the data at wire speed. Since sequential flash access outperforms FlashBoost
with two threads, it can be seen that this performance difference is not because
of the flash device performance but because of architectural differences and
software overhead.

Using the full bandwidth of the storage system would
have made this gap even more pronounced, as the software's bandwidth would be
limited by the PCIe running at 1.6GB/s. It can be seen that when even most of
the data can fit in DRAM, even rare access into storage can have a significant
impact on performance. These results further reinforces our claim that better
storage systems are required for effective analytics of very large datasets.

Figure~\ref{fig:result_hammingfull} shows the performance of un-throttled
FlashBoost, compared to a DRAM implementation. A single node of FlashBoost
outperforms DRAM up to 4 threads, and two nodes outperform DRAM up to 16
threads.

\subsection{Graph Traversal}

Figure~\ref{fig:result_graph} shows the performance of the graph traversal
accelerator, compared to a software implementation that accesses remote nodes
over a separate network. It can be seen that the low latency access to flash
becomes beneficial.




\subsection{Hardware-Accelerate String Search}

We were able to saturate the bandwidth of the flash store using multiple string
search cores.

This section is being written by Ming and Sungjin

