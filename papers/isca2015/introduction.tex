\section{Introduction}

Complex analytics of so-called "Big Data" has shown to provide extremely deep
insight into real world phenomena, but the sheer size of datasets involved makes
designing efficient analytic systems challenging. Ideally, the entire dataset
should fit in the collective DRAM of a moderately sized cluster. However, DRAM's
high price tag of tens of dollars per GB, and high power consumption makes this
impractical. Assuming a moderate size dataset of 10TBs, the price of the DRAM
alone exceeds hundreds of thousands of dollars. On the other hand, disk storage
costs tens of cents per GB, pricing the same capacity around a few thousand
dollars. However, disk storage's prohibitively high access overhead makes
using cheap disk storage in a random-access intensive analytics workload
difficult. Flash storage provides a middle ground between price and performance. While
flash storage is priced a few times more expensive than disks, it has a low
random access latency of around 100$\mu s$, compared to multiple milliseconds of
hard disks. It also consumes much less power than both DRAM and disk storage.

%Flash research is in two ways: replacing DRAM and replacing Disk. neither are
%perfect answers

Traditionally, flash storage has mostly been used as a plug-in alternative to
disk, and this has caused flash to be used ineffectively. Flash has multiple
differences from disk, such as requiring a high-granularity block erase
before doing a low-granularity page write, having a limited program/erase
cycle per cell, and having uniform latency access to all pages.
However, in order to make flash backwards compatible with disk interfaces,
intermediate layers such as the Flash Translation Layer (FTL) had to be built in
order to make flash fit into a disk interface. This adds overhead and
cost, preventing maximum use of flash device capabilities. Most flash-based
storage devices only deliver a fraction of performance that can be achieved by
the raw flash chips in the product.

%Second, disk optimized software
%Third, architecturally... +network?

Flash storage's performance characteristics also puts it in an interesting spot
for architectural exploration. Flash has low latency and high bandwidth which is
on par with many widely used networking infrastructure technologies. This means
in remote storage access, improving storage or network performance individually
may result in little overall benefit because the bottleneck might simply shift
to the other component. Going through the both storage and network management
software layers also becomes a major overhead.

In this work, we present FlashBoost, which presents a possible solution to this
problem using two architectural improvements: integrated storage area network,
and in-store processing engines. Integrating the storage area network into the
storage device removes the software and host-side interface overhead between
storage and network, allowing a lower latency access to remote data.  
The in-store processing engine can pre-process data inside the storage itself,
parallelizing processing while reducing the amount of data transferred to the
host. This becomes increasingly important with the integrated network, because
the combined bandwidth of the network and storage can easily surpass the
host-side interface.

The major goal of FlashBoost is twofold: to make maximum use of flash device
performance in a distributed analytics setting, such that the other components
of the system does not bottleneck flash. 

Our key contributions in this paper are as follows: We present an alternative
architecture of distributed flash-storage systems, including a integrated
storage-area network and in-store processing engine.
We have demonstrated the performance of such an architecture by building a
cluster system of realistic size and capacity and processing real world scale
amount of data. We compare our results against various other approaches that
could be taken with such a large scale data, and argue that the FlashBoost
architecture is a very good balance between system cost, power consumption and
performance.

%	We built a machine
In order to demonstrate the performance of our architecture, we have constructed
a 20-node FlashBoost cluster with 20TBs of flash storage. Each node consists of
a host server with a 12-core Xeon server and 48GBs of DRAM, and a FlashBoost
storage device built using an off-the-shelf Xilinx VC707 FPGA development board
and two custom-designed flash boards, each with 0.5TBs of flash. All nodes are
connected to each other using both Ethernet between host servers and 8
high-speed serial links directly between the FPGAs. The FPGA communicates with
the host server over high-speed PCIe link, and host software can access the
flash storage and the in-store processing engine over either a RPC-like raw
interface or a file system interface that we have developed. A pair of our flash
boards deliver 3.2GB/s of read bandwidth, and 2.4GB/s of write bandwidth.  The 8
serial links between FPGAs each run at 10Gb/s, and all nodes act as a switch as
well as an endpoint, allowing freedom to explore topologies.

%	Rest of the paper is...
The rest of the paper is organized as follows:
