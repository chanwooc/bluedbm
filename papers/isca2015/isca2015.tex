\documentclass[pageno]{jpaper}

%replace XXX with the submission number you are given from the ISCA submission site.
\newcommand{\iscasubmissionnumber}{XXX}

\usepackage{multirow}
\usepackage{listings}
\usepackage[normalem]{ulem}

\begin{document}

\title{
BlueDBM: An Appliance for Big Data Analytics
}

\author{
	Sang-Woo Jun$^\dagger$\quad Ming Liu$^\dagger$\quad Sungjin Lee$^\dagger$\quad Jamey
	Hicks$^\star$\quad \\
	John
	Ankcorn$^\star$\quad Myron King$^\star$\quad Shuotao Xu$^\dagger$\quad Arvind$^\dagger$ \\
	\\
	Department of Electrical Engineering and Computer Science \\
	Massachusetts Institute of Technology$^\dagger$ \\
	Quanta Research Cambridge$^\star$ \\
  \authemail{\{wjun,ml,chamdoo,shuotao,arvind\}@csail.mit.edu}$^\dagger$ \\
  \authemail{\{jamey.hicks,john.ankcorn,myron.king\}@qrclab.com}$^\star$
}
\date{}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% The ACM Copyright Paragraph must appear on the first page of each 
% paper. Government authors should refer to the alternative copyright
% instructions @ http://www.acm.org/sigs/volunteer_resources/conference_manual/6-5proc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 
\copyrightnotice{
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage, and that copies bear
this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee.\\
\textit{ISCA'15, June 13-17, 2015, Portland, OR USA}\\
Rights management text and bibliographic strip from ACM placed here.
}

\thispagestyle{empty}

\begin{abstract}
Complex data queries, because of their need for random accesses, have proven to
be slow unless all the data can be accommodated in DRAM. There are many domains,
such as genomics, geological data and daily twitter feeds where the datasets of
interest are 5TB to 20 TB. For such a dataset, one would need a cluster with 100
servers, each with 128GB to 256GBs of DRAM, to accommodate all the data in DRAM.
On the other hand, such datasets could be stored easily in the flash memory of a
rack-sized cluster. Flash storage has much better random access performance than
hard disks, which makes it desirable for analytics workloads. In this paper we
present BlueDBM, a new system architecture which has flash-based storage with
in-store processing capability and a low-latency high-throughput
inter-controller network.  We show that BlueDBM outperforms a flash-based system
without these features by a factor of 10 for some important applications. While
the performance of a ram-cloud system falls sharply even if only
5\%\textasciitilde10\% of the references are to the secondary storage, this
sharp performance degradation is not an issue in BlueDBM. BlueDBM presents an attractive point in the cost-performance trade-off for Big Data analytics. 
\end{abstract}


\input{introduction}
%\input{motivation}

\input{related}
\input{architecture}
\input{software}
\input{implementation}
\input{results}
\input{acceleration}
%\input{results_acceleration}

\section{Conclusion and Future Work}
\label{sec:conclusion}

We have presented BlueDBM, an appliance for Big Data analytics that uses
flash storage, in-store processing and integrated networks for cost-effective
analytics of large datasets. A rack-size BlueDBM system is likely to be an
order of magnitude cheaper and less power hungry than a cloud based system with
enough DRAM to accommodate 10TB to 20TB of data. We have demonstrated the
performance benefits of BlueDBM using simple examples on large amounts of
data in comparison to a generic flash-based system without such architectural
improvements. We have also shown that the performance of a system which relies
on data being resident in DRAM, falls rapidly if even a small fraction of data
has to reside in secondary storage. BlueDBM like architecture does not suffer
from this problem because flash based systems with 10TB to 20TB of storage are
very affordable.



Our current implementation uses an FPGA to implement most of the new
architectural features, that is, in-store processors, integrated network
routers, flash controllers. It is straightforward to implement most of these
features using ASICs and provide some in-store computing capability via
general-purpose processors. This will simultaneously improve the performance
and lower the power consumption even further. Notwithstanding such developments
we are developing tools to make it easy to develop in-store processors for the
reconfigurable logic inside BlueDBM.  



We are currently developing or planning to develop several new applications
including:  
\emph{SQL Database Acceleration} by offloading query processing and
filtering to in-store processors,
\emph{Sparse-Matrix Based Linear Algebra Acceleration} and
\emph{BlueDBM-Optimized MapReduce}, which attempts to optimize data
flow of MapReduce to best fit an SSD-based cluster with in-store processors.
We plan to collaborate with other research groups to explore more applications.

%FIXME the intel code is for the big data project. Is this the correct one?
\section{Acknowledgements}
This work was partially funded by Quanta (Agmt. Dtd. 04/01/05), Samsung (Res.
Agmt. Eff. 01/01/12), Lincoln Laboratory (PO7000261350), and Intel (Agmt. Eff.
07/23/12). We also thank Xilinx for
their generous donation of VC707 FPGA boards and FPGA design expertise.

\vfill
%\pagebreak

\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\end{document}


